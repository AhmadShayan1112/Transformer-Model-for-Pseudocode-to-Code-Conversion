{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10884189,"sourceType":"datasetVersion","datasetId":6763385}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ntsv_file = \"/kaggle/input/dataset-for-code-genration/poc-train-train.tsv\"\ncsv_file = \"Train_1.csv\"\ndf = pd.read_csv(tsv_file, sep='\\t')\ndf.to_csv(csv_file, index=False)\ntsv_file = \"/kaggle/input/dataset-for-code-genration/spoc-testp.tsv\"\ncsv_file = \"Train_2.csv\"\ndf = pd.read_csv(tsv_file, sep='\\t')\ndf.to_csv(csv_file, index=False)\ntsv_file = \"/kaggle/input/dataset-for-code-genration/spoc-testw.tsv\"\ncsv_file = \"Train_3.csv\"\ndf = pd.read_csv(tsv_file, sep='\\t')\ndf.to_csv(csv_file, index=False)\ntsv_file = \"/kaggle/input/dataset-for-code-genration/spoc-train-eval.tsv\"\ncsv_file = \"Train_4.csv\"\ndf = pd.read_csv(tsv_file, sep='\\t')\ndf.to_csv(csv_file, index=False)\ntsv_file = \"/kaggle/input/dataset-for-code-genration/spoc-train-test.tsv\"\ncsv_file = \"Train_5.csv\"\ndf = pd.read_csv(tsv_file, sep='\\t')\ndf.to_csv(csv_file, index=False)\ntsv_file = \"/kaggle/input/dataset-for-code-genration/spoc-train.tsv\"\ncsv_file = \"Train_6.csv\"\ndf = pd.read_csv(tsv_file, sep='\\t')\ndf.to_csv(csv_file, index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:56:51.735164Z","iopub.execute_input":"2025-03-01T09:56:51.735584Z","iopub.status.idle":"2025-03-01T09:56:54.882067Z","shell.execute_reply.started":"2025-03-01T09:56:51.735548Z","shell.execute_reply":"2025-03-01T09:56:54.881314Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\ntrain_1 = '/kaggle/working/Train_1.csv' \ntrain_2 = '/kaggle/working/Train_2.csv'\ntrain_3 = '/kaggle/working/Train_3.csv'\ntrain_4 = '/kaggle/working/Train_4.csv'\ntrain_5 = '/kaggle/working/Train_5.csv'\ntrain_6 = '/kaggle/working/Train_6.csv'\ntrain_data_1 = pd.read_csv(train_1)\ntrain_data_2 = pd.read_csv(train_2)\ntrain_data_3 = pd.read_csv(train_3)\ntrain_data_4 = pd.read_csv(train_4)\ntrain_data_5 = pd.read_csv(train_5)\ntrain_data_6 = pd.read_csv(train_6)\nmerged_data = pd.concat([train_data_1, train_data_2, train_data_3,train_data_4,train_data_5,train_data_6], ignore_index=True)\nmerged_file = 'merged_data.csv'\nmerged_data.to_csv(merged_file, index=False)\nprint(f\"Data has been successfully merged and saved to {merged_file}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:03.812533Z","iopub.execute_input":"2025-03-01T09:57:03.812819Z","iopub.status.idle":"2025-03-01T09:57:05.957279Z","shell.execute_reply.started":"2025-03-01T09:57:03.812797Z","shell.execute_reply":"2025-03-01T09:57:05.956498Z"}},"outputs":[{"name":"stdout","text":"Data has been successfully merged and saved to merged_data.csv.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(\"merged_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:05.958361Z","iopub.execute_input":"2025-03-01T09:57:05.958587Z","iopub.status.idle":"2025-03-01T09:57:06.717949Z","shell.execute_reply.started":"2025-03-01T09:57:05.958561Z","shell.execute_reply":"2025-03-01T09:57:06.717301Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Removing Column","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=['workerid'])\ndf = df.drop(columns=['probid'])\ndf = df.drop(columns=['line'])\ndf = df.drop(columns=['indent'])\ndf = df.drop(columns=['subid'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:06.718993Z","iopub.execute_input":"2025-03-01T09:57:06.719192Z","iopub.status.idle":"2025-03-01T09:57:06.840489Z","shell.execute_reply.started":"2025-03-01T09:57:06.719176Z","shell.execute_reply":"2025-03-01T09:57:06.839835Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Text Tokenization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndef tokenize_text(text):\n    if pd.isna(text):\n        return []\n    text = text.replace(',', ' ')\n    return text.split()\ndef tokenize_code(text):\n    if pd.isna(text):\n        return []\n    text = text.replace(',', ' ')\n    return text.split()\n\ndf['tokens'] = df['text'].apply(tokenize_text)\ndf['code_token'] = df['code'].apply(tokenize_code)\ndf = df.drop(columns=['text'])\ndf = df.drop(columns=['code'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:08.246803Z","iopub.execute_input":"2025-03-01T09:57:08.247158Z","iopub.status.idle":"2025-03-01T09:57:11.308602Z","shell.execute_reply.started":"2025-03-01T09:57:08.247133Z","shell.execute_reply":"2025-03-01T09:57:11.307911Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df.to_csv(\"tokenized_dataset.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:11.309699Z","iopub.execute_input":"2025-03-01T09:57:11.309993Z","iopub.status.idle":"2025-03-01T09:57:13.287287Z","shell.execute_reply.started":"2025-03-01T09:57:11.309966Z","shell.execute_reply":"2025-03-01T09:57:13.286620Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Adding Necessary Token","metadata":{}},{"cell_type":"code","source":"def add_start_end_tokens(tokens):\n    return ['<start>'] + tokens + ['<end>']\ndf['code_token'] = df['code_token'].apply(add_start_end_tokens)\nmax_length = max(df['tokens'].apply(len).max(), df['code_token'].apply(len).max())\ndef pad_tokens(tokens, max_length):\n    return tokens + ['<pad>'] * (max_length - len(tokens))\ndf['tokens'] = df['tokens'].apply(lambda x: pad_tokens(x, max_length))\ndf['code_token'] = df['code_token'].apply(lambda x: pad_tokens(x, max_length))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:13.288620Z","iopub.execute_input":"2025-03-01T09:57:13.288847Z","iopub.status.idle":"2025-03-01T09:57:18.845835Z","shell.execute_reply.started":"2025-03-01T09:57:13.288821Z","shell.execute_reply":"2025-03-01T09:57:18.845104Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df.to_csv(\"tokenized_dataset.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:18.846783Z","iopub.execute_input":"2025-03-01T09:57:18.846960Z","iopub.status.idle":"2025-03-01T09:57:38.140175Z","shell.execute_reply.started":"2025-03-01T09:57:18.846943Z","shell.execute_reply":"2025-03-01T09:57:38.139278Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Vocablury","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport ast\ndata_path = \"tokenized_dataset.csv\"\ndf = pd.read_csv(data_path, header=None, names=[\"tokens\", \"code_token\"])\nvocab = set()\nfor col in [\"tokens\", \"code_token\"]:\n    for text in df[col]:\n        try:\n            words = ast.literal_eval(text)  # Safely convert string to list\n            vocab.update(words)\n        except (SyntaxError, ValueError):\n            print(f\"Skipping malformed entry: {text}\")\nspecial_tokens = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<UNK>\": 3}\nvocab.discard(\"<pad>\")\nvocab.discard(\"<start>\")\nvocab.discard(\"<end>\")\nvocab.discard(\"<UNK>\")\nword_to_index = {word: idx for idx, word in enumerate(sorted(vocab), start=4)}\nword_to_index = {**special_tokens, **word_to_index}\njson_path = \"vocabulary.json\"\nwith open(json_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(word_to_index, f, ensure_ascii=False, indent=4)\nprint(f\"Vocabulary saved to {json_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:57:38.141071Z","iopub.execute_input":"2025-03-01T09:57:38.141394Z","iopub.status.idle":"2025-03-01T09:59:39.020236Z","shell.execute_reply.started":"2025-03-01T09:57:38.141365Z","shell.execute_reply":"2025-03-01T09:59:39.019312Z"}},"outputs":[{"name":"stdout","text":"Skipping malformed entry: tokens\nSkipping malformed entry: code_token\nVocabulary saved to vocabulary.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import json\nwith open('vocabulary.json', 'r', encoding='utf-8') as f:\n    vocab_dict = json.load(f)\nword = \"create\"\nword_index = vocab_dict.get(word, None)\nprint(f\"Index of '{word}': {word_index}\")\nreverse_vocab_dict = {v: k for k, v in vocab_dict.items()}\nindices = [3, 4, 5, 6]  # Example word indices\ndecoded_words = [reverse_vocab_dict.get(idx, \"<UNK>\") for idx in indices]\nprint(f\"Decoded words: {' '.join(decoded_words)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:59:39.021165Z","iopub.execute_input":"2025-03-01T09:59:39.021505Z","iopub.status.idle":"2025-03-01T09:59:39.059144Z","shell.execute_reply.started":"2025-03-01T09:59:39.021476Z","shell.execute_reply":"2025-03-01T09:59:39.058294Z"}},"outputs":[{"name":"stdout","text":"Index of 'create': 29038\nDecoded words: <UNK> ! !!((N !!(K\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### Creating Sequence","metadata":{}},{"cell_type":"code","source":"import csv\nimport json\nvocab_path = 'vocabulary.json'\ntokenized_data_path = 'tokenized_dataset.csv'\noutput_csv_path = 'sequences.csv'\nwith open(vocab_path, 'r', encoding='utf-8') as f:\n    vocabulary = json.load(f)\ndef text_to_sequence(text, vocab):\n    sequence = []\n    for word in text:\n        word_cleaned = word.strip()\n        if word_cleaned in vocab:\n            sequence.append(vocab[word_cleaned])\n        else:\n            print(f\"Unmatched token: '{word_cleaned}'\")\n            sequence.append(-1)\n    return sequence\nwith open(tokenized_data_path, 'r', encoding='utf-8') as csvfile:\n    reader = csv.DictReader(csvfile)\n    output_data = []\n    for row in reader:\n        input_text = eval(row['tokens'])\n        output_text = eval(row['code_token'])\n        input_sequence = text_to_sequence(input_text, vocabulary)\n        output_sequence = text_to_sequence(output_text, vocabulary)\n        output_data.append({\n            'tokens': input_sequence,\n            'code_token': output_sequence\n        })\nwith open(output_csv_path, 'w', encoding='utf-8', newline='') as csvfile:\n    fieldnames = ['tokens', 'code_token']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in output_data:\n        writer.writerow({\n            'tokens': row['tokens'],\n            'code_token': row['code_token']\n        })\nprint(f\"Converted sequences have been saved to {output_csv_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T09:59:39.060035Z","iopub.execute_input":"2025-03-01T09:59:39.060349Z","iopub.status.idle":"2025-03-01T10:01:44.424938Z","shell.execute_reply.started":"2025-03-01T09:59:39.060319Z","shell.execute_reply":"2025-03-01T10:01:44.424049Z"}},"outputs":[{"name":"stdout","text":"Converted sequences have been saved to sequences.csv.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Data loader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset,Dataset\nimport pandas as pd\nimport torch\nimport ast\nfrom torch.nn.utils.rnn import pad_sequence\nclass DataLoad(Dataset):\n  def __init__(self, file_path):\n    df = pd.read_csv(file_path)\n    self.inputs = [ast.literal_eval(x) for x in df['tokens']]\n    self.outputs = [ast.literal_eval(x) for x in df['code_token']]\n  def __len__(self):\n    return len(self.inputs)\n  def __getitem__(self,idx):\n    input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n    output_tensor = torch.tensor(self.outputs[idx],dtype=torch.int64)\n    return input_tensor, output_tensor\ndef Add_Pad(batch):\n  inputs,outputs = zip(*batch)\n  inputs = pad_sequence(inputs,batch_first=True,padding_value=0)\n  outputs = pad_sequence(outputs,batch_first=True,padding_value=0)\n  return inputs,outputs\nbatch_size=32\ndataset = DataLoad('sequences.csv')\ndataloader = DataLoader(dataset,batch_size, shuffle=True,collate_fn=Add_Pad)\ndata_iter = iter(dataloader)\nfeatures, labels = next(data_iter)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:01:44.427322Z","iopub.execute_input":"2025-03-01T10:01:44.427542Z","iopub.status.idle":"2025-03-01T10:03:38.468845Z","shell.execute_reply.started":"2025-03-01T10:01:44.427522Z","shell.execute_reply":"2025-03-01T10:03:38.467863Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import json\nwith open('vocabulary.json', 'r', encoding='utf-8') as f:\n    vocab_dict = json.load(f)\nVocab_size = len(vocab_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:03:38.470134Z","iopub.execute_input":"2025-03-01T10:03:38.470470Z","iopub.status.idle":"2025-03-01T10:03:38.502944Z","shell.execute_reply.started":"2025-03-01T10:03:38.470440Z","shell.execute_reply":"2025-03-01T10:03:38.502021Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(\"Features:\\n\", features)\nprint(\"Labels:\\n\", labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:03:38.503976Z","iopub.execute_input":"2025-03-01T10:03:38.504320Z","iopub.status.idle":"2025-03-01T10:03:38.535108Z","shell.execute_reply.started":"2025-03-01T10:03:38.504289Z","shell.execute_reply":"2025-03-01T10:03:38.534158Z"}},"outputs":[{"name":"stdout","text":"Features:\n tensor([[40914, 61359, 23335,  ...,     0,     0,     0],\n        [50800, 42706, 23357,  ...,     0,     0,     0],\n        [30193, 40712, 40712,  ...,     0,     0,     0],\n        ...,\n        [    0,     0,     0,  ...,     0,     0,     0],\n        [30193, 28433, 37805,  ...,     0,     0,     0],\n        [52131, 45942,     0,  ...,     0,     0,     0]])\nLabels:\n tensor([[    1, 34192,  3892,  ...,     0,     0,     0],\n        [    1, 28958, 14169,  ...,     0,     0,     0],\n        [    1, 40712, 40712,  ...,     0,     0,     0],\n        ...,\n        [    1, 65822,     2,  ...,     0,     0,     0],\n        [    1, 28430, 37546,  ...,     0,     0,     0],\n        [    1, 27465, 14288,  ...,     0,     0,     0]])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Adam W optimizer","metadata":{}},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)  # Default weight_decay is 0.01","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transformer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=60):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)].to(x.device)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=512, max_len=60):\n        super(TransformerModel, self).__init__()\n\n        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model, padding_idx=0)\n        self.decoder_embedding = nn.Embedding(output_vocab_size, d_model, padding_idx=0)\n        self.positional_encoding = PositionalEncoding(d_model, max_len)\n\n        self.transformer = nn.Transformer(\n            d_model=d_model, \n            nhead=nhead, \n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers, \n            dim_feedforward=dim_feedforward,\n            dropout=0.1\n        )\n\n        self.fc_out = nn.Linear(d_model, output_vocab_size)\n\n    def generate_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n        return mask.to(device)\n\n    def forward(self, src, tgt):\n        src_mask = None\n        tgt_mask = self.generate_mask(tgt.size(1))\n\n        src_key_padding_mask = src == 0\n        tgt_key_padding_mask = tgt == 0\n\n        src_emb = self.positional_encoding(self.encoder_embedding(src))\n        tgt_emb = self.positional_encoding(self.decoder_embedding(tgt))\n\n        output = self.transformer(\n            src_emb.permute(1, 0, 2), \n            tgt_emb.permute(1, 0, 2),\n            src_mask=src_mask,\n            tgt_mask=tgt_mask,\n            src_key_padding_mask=src_key_padding_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask\n        )\n\n        output = self.fc_out(output.permute(1, 0, 2))\n        return output\ninput_vocab_size = Vocab_size  \noutput_vocab_size = Vocab_size  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerModel(input_vocab_size, output_vocab_size).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(), lr=5e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:03:38.536031Z","iopub.execute_input":"2025-03-01T10:03:38.536353Z","iopub.status.idle":"2025-03-01T10:03:41.125489Z","shell.execute_reply.started":"2025-03-01T10:03:38.536325Z","shell.execute_reply":"2025-03-01T10:03:41.124617Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Make Prediction","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nfrom tqdm import tqdm\n\n# Load vocabulary\nwith open(\"vocabulary.json\", \"r\") as f:\n    vocab = json.load(f)\n\n# Create token-to-index and index-to-token mappings\ntoken2idx = vocab\nidx2token = {idx: token for token, idx in vocab.items()}\n\ndef tokenize(sentence, token2idx):\n    \"\"\"Tokenizes the input sentence using the vocabulary.\"\"\"\n    return [token2idx.get(token, token2idx[\"<UNK>\"]) for token in sentence.split()]\n\ndef detokenize(indices, idx2token):\n    \"\"\"Converts token indices back to words.\"\"\"\n    return \" \".join([idx2token.get(idx, \"<UNK>\") for idx in indices])\n\ndef predict(model, sentence, max_len=60):\n    \"\"\"Generates C++ code from pseudocode or vice versa.\"\"\"\n    model.eval()\n    \n    # Tokenize input and convert to tensor\n    input_tokens = tokenize(sentence, token2idx)\n    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n\n    # Start with the <sos> token for decoding\n    output_tokens = [token2idx[\"<start>\"]]\n    \n    with torch.no_grad():\n        for _ in tqdm(range(max_len), desc=\"Decoding\", leave=True):\n            output_tensor = torch.tensor(output_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n\n            # Generate next token\n            predictions = model(input_tensor, output_tensor)  # Shape: (1, seq_len, vocab_size)\n            next_token = predictions[0, -1].argmax(dim=-1).item()\n\n            # Stop if <eos> is generated\n            if next_token == token2idx[\"<end>\"]:\n                break\n\n            # Append next token to output sequence\n            output_tokens.append(next_token)\n\n    # Convert token indices back to words\n    return detokenize(output_tokens[1:], idx2token)  # Exclude <sos> token\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:03:41.126369Z","iopub.execute_input":"2025-03-01T10:03:41.126712Z","iopub.status.idle":"2025-03-01T10:03:41.175721Z","shell.execute_reply.started":"2025-03-01T10:03:41.126693Z","shell.execute_reply":"2025-03-01T10:03:41.175075Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Traning the from the pretrained model and saving after each epoch","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport os\nfrom tqdm import tqdm\n\n# Define training parameters\nnum_epochs = 10\nclip_grad = 1.0  # Gradient clipping\nbatch_size = 10  # Adjust as needed\nmodel_save_path = \"transformer_model.pt\"  # File to save/load model\n\n# Move model to device\nmodel.to(device)\n\n# Try to load existing trained model\nif os.path.exists(model_save_path):\n    print(f\"üîÑ Loading pre-trained model from '{model_save_path}'...\")\n    checkpoint = torch.load(model_save_path, map_location=device)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    print(\"‚úÖ Model loaded successfully! Continuing training...\\n\")\nelse:\n    print(\"‚ö†Ô∏è No pre-trained model found. Training from scratch...\\n\")\n\n# Training function\ndef train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n    model.train()  # Set to training mode\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        epoch_loss = 0\n\n        # Wrap dataloader with tqdm for progress visualization\n        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\")\n\n        for batch_idx, (features, labels) in progress_bar:\n            features, labels = features.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            output = model(features, labels[:, :-1])  # Remove last token from labels\n            output = output.reshape(-1, output_vocab_size)  # Reshape for loss calculation\n            labels = labels[:, 1:].reshape(-1)  # Shift labels by one position\n\n            # Compute loss\n            loss = criterion(output, labels)\n            loss.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n\n            # Update weights\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            # Update tqdm progress bar with current loss\n            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n        end_time = time.time()\n        avg_loss = epoch_loss / len(dataloader)\n        print(f\"\\nEpoch {epoch+1} completed in {end_time - start_time:.2f} sec, Avg Loss: {avg_loss:.4f}\")\n\n        # Save model after each epoch\n        model_save_path = \"transformer_model.pt\"\n        torch.save({\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict()\n        }, model_save_path)\n        print(f\"üíæ Model saved to '{model_save_path}' ‚úÖ\\n\")\n\n        # Run prediction after each epoch\n        manual_input = \"function to add two numbers\"\n        output = predict(model, manual_input)\n        print(f\"\\nGenerated Output after Epoch {epoch+1}:\\n{output}\\n\")\n\n# Start training\ntrain_model(model, dataloader, criterion, optimizer, num_epochs=num_epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:30:35.792943Z","iopub.execute_input":"2025-03-01T11:30:35.793271Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading pre-trained model from 'transformer_model.pt'...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-25-7b8926026d97>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_save_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Model loaded successfully! Continuing training...\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21084/21084 [23:52<00:00, 14.72it/s, loss=1.7024]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 completed in 1432.39 sec, Avg Loss: 0.6631\nüíæ Model saved to 'transformer_model.pt' ‚úÖ\n\n","output_type":"stream"},{"name":"stderr","text":"Decoding:   2%|‚ñè         | 1/60 [00:00<00:00, 67.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Output after Epoch 1:\ntwo++;\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21084/21084 [23:20<00:00, 15.06it/s, loss=0.2624]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 completed in 1400.31 sec, Avg Loss: 0.5661\nüíæ Model saved to 'transformer_model.pt' ‚úÖ\n\n","output_type":"stream"},{"name":"stderr","text":"Decoding:   2%|‚ñè         | 1/60 [00:00<00:00, 65.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Output after Epoch 2:\ntwo++;\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21084/21084 [23:20<00:00, 15.05it/s, loss=0.2510]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 completed in 1400.77 sec, Avg Loss: 0.5660\nüíæ Model saved to 'transformer_model.pt' ‚úÖ\n\n","output_type":"stream"},{"name":"stderr","text":"Decoding:   2%|‚ñè         | 1/60 [00:00<00:00, 67.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Output after Epoch 3:\ntwo++;\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21084/21084 [23:21<00:00, 15.05it/s, loss=0.5933]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 completed in 1401.24 sec, Avg Loss: 0.5659\nüíæ Model saved to 'transformer_model.pt' ‚úÖ\n\n","output_type":"stream"},{"name":"stderr","text":"Decoding:   2%|‚ñè         | 1/60 [00:00<00:00, 65.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Output after Epoch 4:\ntwo++;\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  20%|‚ñà‚ñâ        | 4177/21084 [04:37<18:43, 15.04it/s, loss=0.7065]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Traning Loop","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nfrom tqdm import tqdm\n\n# Define training parameters\nnum_epochs = 10\nclip_grad = 1.0  # Gradient clipping\nbatch_size = 10  # Adjust as needed\n\n# Move model to device\nmodel.to(device)\n\n# Training function\ndef train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n    model.train()  # Set to training mode\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        epoch_loss = 0\n\n        # Wrap dataloader with tqdm for progress visualization\n        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\")\n\n        for batch_idx, (features, labels) in progress_bar:\n            features, labels = features.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            # Forward pass\n            output = model(features, labels[:, :-1])  # Remove last token from labels\n            output = output.reshape(-1, output_vocab_size)  # Reshape for loss calculation\n            labels = labels[:, 1:].reshape(-1)  # Shift labels by one position\n\n            # Compute loss\n            loss = criterion(output, labels)\n            loss.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n\n            # Update weights\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            # Update tqdm progress bar with current loss\n            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n        end_time = time.time()\n        avg_loss = epoch_loss / len(dataloader)\n        print(f\"\\nEpoch {epoch+1} completed in {end_time - start_time:.2f} sec, Avg Loss: {avg_loss:.4f}\")\n\n        # Run prediction after each epoch\n        manual_input = \"function to add two numbers\"\n        output = predict(model, manual_input)\n        print(f\"\\nGenerated Output after Epoch {epoch+1}:\\n{output}\\n\")\ntrain_model(model, dataloader, criterion, optimizer, num_epochs=num_epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T10:03:41.176421Z","iopub.execute_input":"2025-03-01T10:03:41.176617Z","iopub.status.idle":"2025-03-01T10:55:58.630020Z","shell.execute_reply.started":"2025-03-01T10:03:41.176599Z","shell.execute_reply":"2025-03-01T10:55:58.628859Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21084/21084 [26:18<00:00, 13.35it/s, loss=1.1361]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 completed in 1578.78 sec, Avg Loss: 1.2868\n","output_type":"stream"},{"name":"stderr","text":"Decoding:   2%|‚ñè         | 1/60 [00:00<00:03, 18.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Output after Epoch 1:\ngetline(cin\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21084/21084 [25:45<00:00, 13.64it/s, loss=0.1356]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 completed in 1545.56 sec, Avg Loss: 0.7745\n","output_type":"stream"},{"name":"stderr","text":"Decoding:   5%|‚ñå         | 3/60 [00:00<00:00, 107.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nGenerated Output after Epoch 2:\ntime += read();\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:   1%|          | 176/21084 [00:12<25:42, 13.55it/s, loss=0.2359]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-ac06dc73b501>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nGenerated Output after Epoch {epoch+1}:\\n{output}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-ac06dc73b501>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Update tqdm progress bar with current loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"### Save Model","metadata":{}},{"cell_type":"code","source":"import torch\nmodel_save_path = \"transformer_model.pt\"\ntorch.save({\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict()\n}, model_save_path)\nprint(f\"‚úÖ Model saved successfully at {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:30:08.920272Z","iopub.execute_input":"2025-03-01T11:30:08.920569Z","iopub.status.idle":"2025-03-01T11:30:09.590516Z","shell.execute_reply.started":"2025-03-01T11:30:08.920548Z","shell.execute_reply":"2025-03-01T11:30:09.589629Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model saved successfully at transformer_model.pt\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Load Model To Make Prediction","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nfrom tqdm import tqdm\n\n# Load vocabulary\nwith open(\"vocabulary.json\", \"r\") as f:\n    vocab = json.load(f)\n\n# Create token-to-index and index-to-token mappings\ntoken2idx = vocab\nidx2token = {idx: token for token, idx in vocab.items()}\n\n# Initialize model_2 (ensure it's the same architecture as the saved model)\nmodel_2 = TransformerModel(input_vocab_size, output_vocab_size).to(device)  # Initialize with same parameters as trained model\noptimizer = torch.optim.Adam(model_2.parameters(), lr=0.001)  # Use same optimizer\n\n# Function to load the model\ndef load_model(model_path, model_2, optimizer):\n    checkpoint = torch.load(model_path, map_location=device)\n    model_2.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    model_2.eval()  # Set to evaluation mode\n    print(f\"‚úÖ Model loaded from {model_path}\")\n\n# Load the saved model\nload_model(\"transformer_model.pt\", model_2, optimizer)\n\n# Tokenization functions\ndef tokenize(sentence, token2idx):\n    \"\"\"Tokenizes the input sentence using the vocabulary.\"\"\"\n    return [token2idx.get(token, token2idx[\"<UNK>\"]) for token in sentence.split()]\n\ndef detokenize(indices, idx2token):\n    \"\"\"Converts token indices back to words.\"\"\"\n    return \" \".join([idx2token.get(idx, \"<UNK>\") for idx in indices])\n\n# Prediction function\ndef predict(model, sentence, max_len=60):\n    \"\"\"Generates C++ code from pseudocode or vice versa.\"\"\"\n    model.eval()\n    \n    # Tokenize input and convert to tensor\n    input_tokens = tokenize(sentence, token2idx)\n    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n\n    # Start with the <sos> token for decoding\n    output_tokens = [token2idx[\"<start>\"]]\n    \n    with torch.no_grad():\n        for _ in tqdm(range(max_len), desc=\"Decoding\", leave=True):\n            output_tensor = torch.tensor(output_tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n\n            # Generate next token\n            predictions = model(input_tensor, output_tensor)  # Shape: (1, seq_len, vocab_size)\n            next_token = predictions[0, -1].argmax(dim=-1).item()\n\n            # Stop if <eos> is generated\n            if next_token == token2idx[\"<end>\"]:\n                break\n\n            # Append next token to output sequence\n            output_tokens.append(next_token)\n\n    # Convert token indices back to words\n    return detokenize(output_tokens[1:], idx2token)  # Exclude <sos> token\n\n# Example input\nmanual_input = 'print a'\noutput = predict(model_2, manual_input)\n\nprint(\"\\nGenerated Output:\\n\", output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T11:28:46.432767Z","iopub.execute_input":"2025-03-01T11:28:46.433096Z","iopub.status.idle":"2025-03-01T11:28:47.268082Z","shell.execute_reply.started":"2025-03-01T11:28:46.433074Z","shell.execute_reply":"2025-03-01T11:28:47.267006Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n<ipython-input-23-66d8776a121c>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-66d8776a121c>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transformer_model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Tokenization functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-66d8776a121c>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_path, model_2, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"],"ename":"KeyError","evalue":"'model_state_dict'","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example input\nmanual_input = \"create string s\"\noutput = predict(model, manual_input)\n\nprint(\"\\nGenerated Output:\\n\", output)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}